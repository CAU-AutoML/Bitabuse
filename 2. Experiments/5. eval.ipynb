{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\"./outputs/bitabuse_infer_ocr_simchar.csv\", index_col=\"id\"),\n",
    "        pd.read_csv(\"./outputs/bitabuse_infer_spell.csv\", index_col=\"id\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from OpenAttack.metric import BLEU, JaccardWord\n",
    "from OpenAttack.text_process.tokenizer import PunctTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils import word_accuracy\n",
    "\n",
    "tokenizer = PunctTokenizer()\n",
    "\n",
    "bleu = BLEU(tokenizer)\n",
    "jaccard_word = JaccardWord(tokenizer)\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row[\"text\"]\n",
    "    label = row[\"label\"]\n",
    "\n",
    "    ocr = row[\"ocr\"]\n",
    "    df.loc[i, \"ocr_word_accuracy\"] = word_accuracy(ocr, label, text)\n",
    "    df.loc[i, \"ocr_bleu\"] = bleu.calc_score(ocr, label)\n",
    "    df.loc[i, \"ocr_jaccard_word\"] = jaccard_word.calc_score(ocr, label)\n",
    "\n",
    "    simchar = row[\"simchar\"]\n",
    "    df.loc[i, \"simchar_word_accuracy\"] = word_accuracy(simchar, label, text)\n",
    "    df.loc[i, \"simchar_bleu\"] = bleu.calc_score(simchar, label)\n",
    "    df.loc[i, \"simchar_jaccard_word\"] = jaccard_word.calc_score(simchar, label)\n",
    "\n",
    "    spell = row[\"spellchecker\"]\n",
    "    df.loc[i, \"spell_word_accuracy\"] = word_accuracy(spell, label, text)\n",
    "    spell_remove_underbar = spell.replace(\"_\", \"\")\n",
    "    df.loc[i, \"spell_bleu\"] = bleu.calc_score(spell_remove_underbar, label)\n",
    "    df.loc[i, \"spell_jaccard_word\"] = jaccard_word.calc_score(\n",
    "        spell_remove_underbar, label\n",
    "    )\n",
    "df.to_csv(\"./outputs/bitabuse_infer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the results of the gpt-4o-mini model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"gpt-4o-mini/gpt_batches\"\n",
    "\n",
    "output_files = [\n",
    "    os.path.join(output_dir, path)\n",
    "    for path in os.listdir(output_dir)\n",
    "    if \"output\" in path\n",
    "]\n",
    "output_files.sort(key=lambda x: int(x.replace(output_dir, \"\").split(\"_\")[2]))\n",
    "output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for path in output_files:\n",
    "    print(path)\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    batch_outputs = []\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        id = int(data[\"custom_id\"].split(\"_\")[1])\n",
    "        output = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        batch_outputs.append((id, output))\n",
    "    batch_outputs.sort(key=lambda x: x[0])\n",
    "    outputs.extend([x[1] for x in batch_outputs])\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from OpenAttack.metric import BLEU, JaccardWord\n",
    "from OpenAttack.text_process.tokenizer import PunctTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils import word_accuracy\n",
    "\n",
    "tokenizer = PunctTokenizer()\n",
    "\n",
    "bleu = BLEU(tokenizer)\n",
    "jaccard_word = JaccardWord(tokenizer)\n",
    "\n",
    "df = pd.read_csv(\"./outputs/bitabuse_infer.csv\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    text = row[\"text\"]\n",
    "    label = row[\"label\"]\n",
    "    gpt4 = outputs[i].lower()\n",
    "\n",
    "    df.loc[i, \"gpt-4o-mini\"] = gpt4\n",
    "    df.loc[i, \"gpt-4o-mini_word_accuracy\"] = word_accuracy(gpt4, label, text)\n",
    "    df.loc[i, \"gpt-4o-mini_bleu\"] = bleu.calc_score(gpt4, label)\n",
    "    df.loc[i, \"gpt-4o-mini_jaccard_word\"] = jaccard_word.calc_score(gpt4, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./outputs/bitabuse_infer_with_gpt4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from src.dataset import train_valid_test_split\n",
    "\n",
    "bitcore = load_dataset(\"AutoML/bitcore\", split=\"train\")\n",
    "bitviper = load_dataset(\"AutoML/bitviper\", split=\"train\")\n",
    "bitabuse = load_dataset(\"AutoML/bitabuse\", split=\"train\")\n",
    "\n",
    "bitcore_ids = []\n",
    "bitviper_ids = []\n",
    "bitabuse_ids = []\n",
    "\n",
    "for seed in range(10):\n",
    "    bitcore_ids.append(\n",
    "        train_valid_test_split(bitcore, split_ratio=(6, 2, 2), seed=seed)[2][\"id\"]\n",
    "    )\n",
    "    bitviper_ids.append(\n",
    "        train_valid_test_split(bitviper, split_ratio=(6, 2, 2), seed=seed)[2][\"id\"]\n",
    "    )\n",
    "    bitabuse_ids.append(\n",
    "        train_valid_test_split(bitabuse, split_ratio=(6, 2, 2), seed=seed)[2][\"id\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./outputs/bitabuse_infer_with_gpt4.csv\", index_col=\"id\")\n",
    "\n",
    "ds_infos = [\n",
    "    (\"BitCore\", bitcore_ids),\n",
    "    (\"BitViper\", bitviper_ids),\n",
    "    (\"BitAbuse\", bitabuse_ids),\n",
    "]\n",
    "metrics = [\"word_accuracy\", \"jaccard_word\", \"bleu\"]\n",
    "method_infos = [\n",
    "    (\"SimChar DB\", \"simchar\"),\n",
    "    (\"OCR\", \"ocr\"),\n",
    "    (\"Spell Checker\", \"spell\"),\n",
    "    (\"GPT-4o-mini\", \"gpt-4o-mini\"),\n",
    "]\n",
    "\n",
    "df_result = pd.DataFrame(\n",
    "    {\n",
    "        \"Measure\": [\"word_accuracy\"] * 3 + [\"jaccard_word\"] * 3 + [\"bleu\"] * 3,\n",
    "        \"Dataset\": [\"BitCore\", \"BitViper\", \"BitAbuse\"] * 3,\n",
    "    }\n",
    ")\n",
    "\n",
    "df_result.set_index([\"Measure\", \"Dataset\"], inplace=True)\n",
    "for ds_name, ids in ds_infos:\n",
    "    for method, col in method_infos:\n",
    "        for metric in metrics:\n",
    "            t = [np.mean(df.loc[id, f\"{col}_{metric}\"]) for id in ids]\n",
    "            tt = rf\"${np.mean(t).round(4):.4f} \\pm {np.std(t).round(4):.4f}$\"\n",
    "            df_result.loc[(metric, ds_name), method] = tt\n",
    "\n",
    "print(df_result.to_latex())\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_names = [\"BitCore\", \"BitViper\", \"BitAbuse\"]\n",
    "\n",
    "ds = {\n",
    "    \"BitCore\": load_dataset(\"AutoML/bitcore\", split=\"train\")\n",
    "    .to_pandas()\n",
    "    .set_index(\"id\"),\n",
    "    \"BitViper\": load_dataset(\"AutoML/bitviper\", split=\"train\")\n",
    "    .to_pandas()\n",
    "    .set_index(\"id\"),\n",
    "    \"BitAbuse\": load_dataset(\"AutoML/bitabuse\", split=\"train\")\n",
    "    .to_pandas()\n",
    "    .set_index(\"id\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import numpy as np\n",
    "from OpenAttack.metric import BLEU, JaccardWord\n",
    "from OpenAttack.text_process.tokenizer import PunctTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.utils import word_accuracy\n",
    "\n",
    "tokenizer = PunctTokenizer()\n",
    "bleu = BLEU(tokenizer)\n",
    "jaccard_word = JaccardWord(tokenizer)\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    for seed in range(10):\n",
    "        df = pd.read_csv(\n",
    "            f\"./outputs/charbert-{ds_name.lower()}/{seed}.csv\", index_col=\"id\"\n",
    "        )\n",
    "        for id, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            text = ds[ds_name].loc[id, \"text\"]\n",
    "            label = ds[ds_name].loc[id, \"label\"]\n",
    "            pred = row[\"charbert\"]\n",
    "            df.loc[id, \"word_accuracy\"] = word_accuracy(pred, label, text)\n",
    "            df.loc[id, \"bleu\"] = bleu.calc_score(pred, label)\n",
    "            df.loc[id, \"jaccard_word\"] = jaccard_word.calc_score(pred, label)\n",
    "        df.to_csv(f\"./outputs/charbert-{ds_name.lower()}/{seed}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Measure\": [\"word_accuracy\"] * 3 + [\"jaccard_word\"] * 3 + [\"bleu\"] * 3,\n",
    "        \"Dataset\": [\"BitCore\", \"BitViper\", \"BitAbuse\"] * 3,\n",
    "    }\n",
    ")\n",
    "result_df.set_index([\"Measure\", \"Dataset\"], inplace=True)\n",
    "\n",
    "for ds_name in ds_names:\n",
    "    for metric in metrics:\n",
    "        t = [\n",
    "            np.mean(\n",
    "                pd.read_csv(\n",
    "                    f\"./outputs/charbert-{ds_name.lower()}/{seed}.csv\",\n",
    "                    index_col=\"id\",\n",
    "                )[metric]\n",
    "            )\n",
    "            for seed in range(10)\n",
    "        ]\n",
    "        tt = rf\"${np.mean(t).round(4):.4f} \\pm {np.std(t).round(4):.4f}$\"\n",
    "        result_df.loc[(metric, ds_name), \"CharBERT\"] = tt\n",
    "\n",
    "print(result_df.to_latex())\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
